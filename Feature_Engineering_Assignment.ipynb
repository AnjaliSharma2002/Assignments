{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6YQmfUxJIGjmLzutuTP3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnjaliSharma2002/Assignments/blob/main/Feature_Engineering_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KdWXEH9ggw84"
      },
      "outputs": [],
      "source": [
        "# Q1.\n",
        "\n",
        "# In Machine Learning (ML), a parameter refers to a variable that the model learns from the training data. These parameters define the model's behavior and help it make predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.\n",
        "\n",
        "# Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It helps determine how changes in one variable are associated with changes in another.\n",
        "\n",
        "# A negative correlation means that as one variable increases, the other decreases."
      ],
      "metadata": {
        "id": "N0TjhYEAhAa4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3.\n",
        "\n",
        "# Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computers to learn from data and make decisions or predictions without being explicitly programmed. It uses mathematical models and algorithms to identify patterns and improve performance over time.\n",
        "\n",
        "# Main Components of Machine Learning:\n",
        "\n",
        "# Data – The foundation of ML, includes training and testing datasets.\n",
        "# Features (Input Variables) – Relevant data attributes used for prediction.\n",
        "# Model – The mathematical function mapping inputs to outputs.\n",
        "# Training Process – Learning patterns from data by adjusting parameters.\n",
        "# Parameters – Model-learned values (e.g., weights in neural networks).\n",
        "# Hyperparameters – Manually set values controlling training (e.g., learning rate).\n",
        "# Loss Function – Measures how far predictions are from actual values.\n",
        "# Optimization Algorithm – Adjusts parameters to minimize loss (e.g., Gradient Descent).\n",
        "# Evaluation Metrics – Assesses model performance (e.g., Accuracy, MSE, F1-score).\n",
        "# Deployment & Monitoring – Deploying models and tracking their real-world performance."
      ],
      "metadata": {
        "id": "54Gh3lO1hd7j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4.\n",
        "\n",
        "# The loss value measures how far off a model's predictions are from the actual values. It plays a crucial role in determining whether a model is good or needs improvement.\n",
        "\n",
        "# A lower loss value suggests that the model is making more accurate predictions, while a higher loss indicates poor performance. During training, optimization algorithms like Gradient Descent adjust model parameters to minimize the loss, improving accuracy over time."
      ],
      "metadata": {
        "id": "8yB9HH1lh_bL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5.\n",
        "\n",
        "# A continuous variable is a numerical variable that can take an infinite number of values within a given range, such as height, weight, temperature, or income. These variables are measurable and allow for arithmetic operations like addition and subtraction. In contrast, a categorical variable represents distinct groups or categories without a meaningful numerical relationship. Categorical variables can be nominal (unordered, like colors or gender) or ordinal (ordered, like education levels or satisfaction ratings). While continuous variables are used for regression models, categorical variables often require encoding techniques like one-hot encoding before being used in machine learning models.\n"
      ],
      "metadata": {
        "id": "LTZjnD_giZx4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6.\n",
        "\n",
        "# Categorical variables need to be converted into numerical representations before being used in ML models. This process is called encoding. There are several techniques for handling categorical variables, depending on whether they are nominal (unordered) or ordinal (ordered).\n",
        "\n",
        "# Common Techniques for Handling Categorical Variables in Machine Learning\n",
        "\n",
        "# One-Hot Encoding (OHE) – Converts each category into a separate binary column (0 or 1). Best for nominal variables with a small number of unique categories.\n",
        "\n",
        "# Label Encoding – Assigns a unique numeric value to each category. Suitable for ordinal variables where order matters.\n",
        "\n",
        "# Ordinal Encoding – Similar to label encoding but explicitly preserves the ranking of categories, making it ideal for ordered data like education levels or satisfaction ratings.\n",
        "\n",
        "# Target Encoding (Mean Encoding) – Replaces categories with the mean of the target variable for that category, useful when a categorical feature has a strong correlation with the target.\n",
        "\n",
        "# Frequency Encoding – Assigns a category a number based on how often it appears in the dataset, helpful for large categorical variables with many unique values."
      ],
      "metadata": {
        "id": "_xi1MboSitPr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7.\n",
        "\n",
        "# Training and testing a dataset in machine learning refers to the process of splitting data into two parts to build and evaluate a model. The training dataset is used to teach the model by identifying patterns and learning from input data. The testing dataset is kept separate and used to assess the model’s performance on unseen data, ensuring it generalizes well. This approach helps prevent overfitting and provides an unbiased evaluation of the model’s accuracy. Common split ratios include 80% training / 20% testing or 70% training / 30% testing, depending on dataset size and complexity."
      ],
      "metadata": {
        "id": "jU5-qwi6jJYN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8.\n",
        "\n",
        "# sklearn.preprocessing is a module in Scikit-Learn that provides various techniques for feature scaling, transformation, and encoding to prepare data for machine learning models. It helps improve model performance by normalizing, standardizing, and encoding input features.\n",
        "\n"
      ],
      "metadata": {
        "id": "MBIKvo2TjVEt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9.\n",
        "\n",
        "# A test set is a subset of the dataset that is used to evaluate the performance of a trained machine learning model. It consists of unseen data that was not used during training, allowing us to assess how well the model generalizes to new inputs.\n"
      ],
      "metadata": {
        "id": "W5iF3Hi9jqgs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10.\n",
        "\n",
        "# In Python, we use train_test_split from sklearn.model_selection to split the dataset into training and testing sets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
        "y = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Set:\", X_train, y_train)\n",
        "print(\"Test Set:\", X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKQfUZXKjsxj",
        "outputId": "c2e932c6-4c40-4337-8f07-040461431495"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: [[6], [1], [8], [3], [10], [5], [4], [7]] [60, 10, 80, 30, 100, 50, 40, 70]\n",
            "Test Set: [[9], [2]] [90, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach to a Machine Learning Problem :\n",
        "\n",
        "# Define the Problem – Understand the goal and identify whether it's classification, regression, or clustering.\n",
        "\n",
        "# Collect & Explore Data – Gather the dataset, handle missing values, and perform Exploratory Data Analysis (EDA).\n",
        "\n",
        "# Preprocess the Data – Encode categorical variables, normalize/standardize features, and remove irrelevant data.\n",
        "\n",
        "# Split the Data – Use train_test_split() to divide data into training and testing sets (e.g., 80-20 split).\n",
        "\n",
        "# Choose & Train a Model – Select an appropriate ML algorithm (e.g., Decision Tree, Neural Network) and train it.\n",
        "\n",
        "# Evaluate the Model – Measure performance using accuracy, precision, recall, MSE, RMSE, or F1-score.\n",
        "\n",
        "# Tune Hyperparameters – Optimize model parameters using GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "# Deploy & Monitor – Deploy the model and monitor its performance in real-world use cases."
      ],
      "metadata": {
        "id": "qTX-rKEbj-r0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11.\n",
        "\n",
        "# EDA (Exploratory Data Analysis) is a crucial step before training a machine learning model because it helps understand the dataset, identify issues, and make informed preprocessing decisions.\n"
      ],
      "metadata": {
        "id": "AywpFjFQkRUx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q12.\n",
        "\n",
        "# Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how one variable changes in relation to another.\n"
      ],
      "metadata": {
        "id": "3bOwYoczkZ5W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13.\n",
        "\n",
        "# A negative correlation means that as one variable increases, the other variable decreases. In other words, they move in opposite directions.\n"
      ],
      "metadata": {
        "id": "Ag7mxmHUki8A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14.\n",
        "\n",
        "# To find the correlation between variables:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {'Temperature': [10, 15, 20, 25, 30],\n",
        "        'Sales': [200, 250, 300, 350, 400]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df.corr())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpZIkjylks9t",
        "outputId": "69b99fb7-b288-4ee8-d0bd-0f169afa2814"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Temperature  Sales\n",
            "Temperature          1.0    1.0\n",
            "Sales                1.0    1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15.\n",
        "\n",
        "# Causation means that one event directly influences another. If A causes B, then changing A will result in a change in B.\n",
        "\n",
        "\n",
        "# Correlation is a statistical relationship between two variables, where they move together in some way. However, correlation does not imply that one variable directly affects the other.\n",
        "\n",
        "# Causation (or causal relationship) means that one event directly influences another. In this case, a change in one variable directly results in a change in the other. Establishing causation requires experimental or observational evidence beyond statistical correlation.\n",
        "\n",
        "\n",
        "# Example: Ice Cream Sales & Drowning Cases\n",
        "# Correlation: Data may show that ice cream sales and drowning cases increase together.\n",
        "\n",
        "# Does this mean eating ice cream causes drowning? ❌ No!\n",
        "\n",
        "# Actual Cause: Hot weather increases both swimming activity (risk of drowning) and ice cream consumption."
      ],
      "metadata": {
        "id": "QU_AT9MclMGo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16.\n",
        "\n",
        "# An optimizer is an algorithm used to adjust the parameters (weights and biases) of a machine learning model to minimize the loss function and improve model performance. It updates parameters iteratively based on gradients computed using techniques like gradient descent.\n",
        "\n",
        "# Types of Optimizers in Machine Learning\n",
        "\n",
        "# 1. Gradient Descent (GD)\n",
        "# Basic optimization algorithm that updates weights by moving in the direction of the negative gradient.\n",
        "\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "model = SGDRegressor(learning_rate='constant', eta0=0.01)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 2. Momentum Optimization\n",
        "# Uses a moving average of past gradients to speed up learning.\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "# 3. Adagrad (Adaptive Gradient Algorithm)\n",
        "# Adapts learning rate individually for each parameter, useful for sparse data.\n",
        "\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "\n",
        "\n",
        "# 4. RMSprop (Root Mean Square Propagation)\n",
        "# Fixes Adagrad’s issue by using a moving average of squared gradients.\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "\n",
        "\n",
        "# 5. Adam (Adaptive Moment Estimation)\n",
        "# Combines Momentum + RMSprop, making it efficient and widely used.\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# 6. AdaDelta\n",
        "# Improves Adagrad by restricting the accumulated past squared gradients.\n",
        "\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "\n",
        "optimizer = Adadelta(learning_rate=1.0)\n"
      ],
      "metadata": {
        "id": "6G9XqVb9lPxf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17.\n",
        "\n",
        "# sklearn.linear_model is a module in Scikit-Learn that provides various linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "Oi1JeUNflTY-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18.\n",
        "\n",
        "# model.fit() is a method used to train a machine learning model by learning patterns from the input data (X) and corresponding target labels (y). It optimizes the model’s parameters (weights and biases) to minimize the loss function.\n",
        "\n",
        "# model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "CGBC1mDSm19f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19.\n",
        "\n",
        "# model.predict() is a method used to make predictions on new or unseen data after a machine learning model has been trained. It takes input features and outputs predicted values based on the learned patterns.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on new data\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EonIsV5nJrg",
        "outputId": "9af0f5b6-e4f5-4c3f-96b6-0d1a56a474d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90. 20.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20.\n",
        "\n",
        "# Continuous Variables are numerical variables that can take an infinite number of values within a given range. They are measured, not counted.\n",
        "# Example: Height, weight, temperature.\n",
        "\n",
        "# Categorical Variables represent distinct groups or categories and do not have a numerical meaning. They can be nominal (unordered) or ordinal (ordered).\n",
        "# Example: Gender, car brand, customer satisfaction level."
      ],
      "metadata": {
        "id": "4l_IL5eEnWxd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21.\n",
        "\n",
        "# Feature scaling is a data preprocessing technique used in Machine Learning to normalize or standardize the range of independent variables (features). It ensures that all features have a comparable scale, preventing certain features from dominating the model due to larger numerical values.\n",
        "\n",
        "# How Feature Scaling Helps:\n",
        "\n",
        "# Prevents Bias – Avoids models favoring features with large magnitudes.\n",
        "\n",
        "# Improves Convergence – Helps gradient-based optimization (e.g., Gradient Descent) train faster.\n",
        "\n",
        "# Enhances Model Performance – Ensures better distance calculations in algorithms like KNN and SVM."
      ],
      "metadata": {
        "id": "EkD5HQ6WntRP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22.\n",
        "\n",
        "# Feature scaling is a technique used to normalize or standardize data so that all features contribute equally to a machine learning model. In Python, scaling is performed using Scikit-Learn’s preprocessing module, which provides methods like Min-Max Scaling, Standardization, and Robust Scaling to transform numerical features.\n",
        "\n"
      ],
      "metadata": {
        "id": "1RZGIFyWoBiV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23.\n",
        "\n",
        "# sklearn.preprocessing is a module in Scikit-Learn that provides tools for data preprocessing before training a machine learning model. It includes methods for feature scaling, encoding categorical variables, handling missing values, and transforming data to improve model performance.\n"
      ],
      "metadata": {
        "id": "j7555V1voSGH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24.\n",
        "# To split data into training and testing sets, we use Scikit-Learn’s train_test_split() function from the sklearn.model_selection module.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
        "y = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Data:\", X_train, y_train)\n",
        "print(\"Testing Data:\", X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BejLKQgnoY8f",
        "outputId": "8f729a34-d2c9-4d09-a7bf-c93012c9f73d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data: [[6], [1], [8], [3], [10], [5], [4], [7]] [60, 10, 80, 30, 100, 50, 40, 70]\n",
            "Testing Data: [[9], [2]] [90, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25.\n",
        "\n",
        "# Data encoding is the process of converting categorical (non-numeric) data into a numerical format that can be understood by machine learning models. Since most ML algorithms work with numerical data, encoding is essential for handling categorical variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "m1BbunoWon6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}