{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8n0EzEh7+XzfdDiw7QEw8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnjaliSharma2002/Assignments/blob/main/Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "71fdV8G800zb"
      },
      "outputs": [],
      "source": [
        "# Q1.\n",
        "\n",
        "# Simple Linear Regression is a statistical technique that models the relationship between a dependent variable and one independent variable by fitting a straight line (called the regression line) through the data points. This line is used to predict the value of the dependent variable based on the given independent variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.\n",
        "\n",
        "# The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "# Linearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "\n",
        "# Independence: Observations (data points) are independent of each other.\n",
        "\n",
        "# Homoscedasticity: The variance of residuals (errors) is constant across all levels of the independent variable.\n",
        "\n",
        "# Normality of Errors: The residuals (differences between actual and predicted values) are normally distributed.\n",
        "\n",
        "# No Multicollinearity (only relevant in multiple regression): Since simple linear regression involves only one predictor, this doesn't apply here."
      ],
      "metadata": {
        "id": "za-J5d_71vv_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3.\n",
        "\n",
        "# In the equation Y=mX+c (which represents a straight line):\n",
        "\n",
        "# m is the coefficient or slope of the line.\n",
        "\n",
        "# It indicates the rate of change of the dependent variable Y with respect to the independent variable X.\n",
        "\n",
        "# In other words, it tells you how much Y changes when X increases by 1 unit."
      ],
      "metadata": {
        "id": "CD0Hhspa2I1i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4.\n",
        "\n",
        "# In the equation Y=mX+c:\n",
        "\n",
        "# c is the intercept, also called the Y-intercept.\n",
        "\n",
        "# What it represents:\n",
        "# It is the value of Y when X=0.\n",
        "\n",
        "# Graphically, it is the point where the line crosses the Y-axis."
      ],
      "metadata": {
        "id": "91ZKg6xQ24IG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5.\n",
        "\n",
        "# In Simple Linear Regression, the slope m (also called ùõΩ1) is calculated using the following formula:\n",
        "\n",
        "# m= (n‚àë(XY)‚àí‚àëX‚àëY)/ n‚àëX^2 ‚àí (‚àëX)^2\n",
        "\n",
        "# Where:\n",
        "# n = number of data points\n",
        "# ‚àëXY = sum of the product of X and Y\n",
        "# ‚àëX = sum of all X values\n",
        "# ‚àëY = sum of all Y values\n",
        "# ‚àëX^2 = sum of squares of X values\n",
        "\n",
        "# It finds the best-fitting line by minimizing the sum of squared differences between the actual values and the predicted values (called least squares method)."
      ],
      "metadata": {
        "id": "d7a_FrDz3VWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6.\n",
        "\n",
        "# The least squares method in Simple Linear Regression is used to determine the best-fitting straight line through a set of data points. Its main purpose is to minimize the sum of the squared differences between the actual values of the dependent variable and the values predicted by the regression line. These differences are called residuals or errors. By squaring them, the method ensures all values are positive and gives more weight to larger errors. Minimizing this total squared error leads to the most accurate estimation of the line's slope and intercept. This approach ensures the regression line represents the overall trend in the data as closely as possible, making it effective for prediction and analysis.\n"
      ],
      "metadata": {
        "id": "vbx5hgX83rGd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7.\n",
        "\n",
        "# In Simple Linear Regression, the coefficient of determination, denoted as R¬≤, indicates how well the independent variable explains the variation in the dependent variable. It is a statistical measure that ranges from 0 to 1, where a value closer to 1 suggests a strong relationship and a good fit of the regression line to the data. For example, an R¬≤ value of 0.90 means that 90% of the variation in the dependent variable can be explained by the independent variable, while the remaining 10% is due to other unknown factors or random variation. In essence, R¬≤ helps us understand the effectiveness of the linear model in capturing the trend present in the data.\n"
      ],
      "metadata": {
        "id": "xgFhCb4s491L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8.\n",
        "\n",
        "# Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between a dependent variable and two or more independent variables. Instead of a single predictor, Multiple Linear Regression allows you to predict the dependent variable based on several predictors.\n",
        "\n",
        "# Equation:\n",
        "# The general form of the equation for Multiple Linear Regression is:\n",
        "\n",
        "# Y=Œ≤0 +Œ≤1 X1+Œ≤2 X2+‚ãØ+Œ≤p Xp+œµ"
      ],
      "metadata": {
        "id": "iPBPLN0G5NyJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9.\n",
        "\n",
        "# The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable. Simple Linear Regression involves only one independent variable (predictor) to model the relationship with the dependent variable, and the equation takes the form Y=Œ≤0 +Œ≤1 X+œµ.\n",
        "\n",
        "# In contrast, Multiple Linear Regression involves two or more independent variables to predict the dependent variable, and the equation is expanded to Y=Œ≤0 +Œ≤1 X1+Œ≤2 X2+‚ãØ+Œ≤p Xp+œµ. While Simple Linear Regression is used when a single factor influences the target variable, Multiple Linear Regression is employed when multiple factors are believed to impact the dependent variable."
      ],
      "metadata": {
        "id": "4sNzM6vq5qf-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10.\n",
        "\n",
        "# The key assumptions of Multiple Linear Regression are critical for ensuring the accuracy and reliability of the model. First, the relationship between the dependent variable and the independent variables must be linear, meaning the change in the dependent variable is a weighted sum of the changes in the independent variables. Second, the independence of errors assumption requires that the residuals (errors) be independent of each other, meaning the error for one observation shouldn't influence another. The homoscedasticity assumption states that the variance of the residuals should remain constant across all levels of the independent variables. Additionally, the residuals should be normally distributed for valid inferences to be made. Another important assumption is no multicollinearity, meaning the independent variables should not be highly correlated with each other, as this could distort the estimation of their individual effects. Finally, no autocorrelation ensures that residuals do not exhibit any patterns, particularly in time series data. These assumptions, if met, allow for a more accurate and interpretable regression model.\n"
      ],
      "metadata": {
        "id": "QL0ZFfmn6RWi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11.\n",
        "\n",
        "# Heteroscedasticity occurs when the variance of the residuals (errors) in a Multiple Linear Regression model is not constant across all levels of the independent variables. This means that as the values of the predictors change, the spread of the residuals also changes. The presence of heteroscedasticity can lead to several issues in the model. Although the regression coefficients remain unbiased, they become inefficient, meaning their standard errors are larger than they would be if homoscedasticity (constant variance) were assumed. This affects the precision of predictions and makes hypothesis tests, such as t-tests and confidence intervals, unreliable. Consequently, this can result in incorrect conclusions about the relationships between variables. Heteroscedasticity can be detected using residual plots or statistical tests like the Breusch-Pagan test, and it can be addressed by applying transformations to the variables, using Weighted Least Squares (WLS), or adjusting the standard errors using robust methods.\n"
      ],
      "metadata": {
        "id": "yijka6KC6kwj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q12.\n",
        "\n",
        "# To improve a Multiple Linear Regression model with high multicollinearity, several approaches can be applied. Multicollinearity occurs when two or more independent variables are highly correlated with each other, making it difficult to estimate the individual effect of each predictor on the dependent variable. One common method to address multicollinearity is removing one of the correlated variables. This can reduce redundancy in the model and improve the accuracy of the coefficient estimates. Another approach is combining correlated variables into a single predictor, such as by calculating their average or creating a composite index. Principal Component Analysis (PCA) is also useful, as it transforms the correlated predictors into a smaller set of uncorrelated components, which can then be used in the regression model. Additionally, regularization techniques like Ridge Regression or Lasso Regression can be applied, which introduce penalties to the model that shrink or eliminate the impact of highly correlated predictors. These methods help improve model stability, prevent overfitting, and lead to more reliable interpretations of the regression results.\n"
      ],
      "metadata": {
        "id": "fQqavJmk6vsJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q13.\n",
        "\n",
        "# When transforming categorical variables for use in regression models, several techniques can be applied depending on the nature of the data. One-Hot Encoding is a popular method that creates binary columns for each category, turning categorical variables into a series of 0s and 1s. This is ideal for nominal variables without any inherent order but can increase dimensionality if there are many categories. Label Encoding assigns a unique integer to each category, which is useful for ordinal variables where the categories have a natural order, though it should be avoided for nominal variables as it may introduce unintended ordinal relationships. Binary Encoding is a hybrid approach that encodes each category as a binary number, reducing dimensionality compared to one-hot encoding, making it more efficient for variables with many categories. Frequency or Count Encoding replaces categories with the number of occurrences of each category, which is simple but may introduce noise if the frequency is not directly related to the target variable. Target Encoding (or mean encoding) uses the mean of the target variable for each category, which can be powerful when there is a strong relationship between the categorical variable and the target, but it requires caution to avoid overfitting. Finally, Ordinal Encoding is used for variables with a natural ranking, assigning numerical values based on the order of the categories. Each of these techniques has its own advantages, and the choice depends on the type of categorical data and the goals of the regression model.\n"
      ],
      "metadata": {
        "id": "oLRQdgdh6-Ao"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14.\n",
        "\n",
        "# In Multiple Linear Regression, interaction terms are used to model the combined effect of two or more independent variables on the dependent variable, where the effect of one predictor depends on the level of another predictor. These terms allow the model to capture more complex relationships that cannot be explained by individual predictors alone. By including interaction terms, the regression model becomes more flexible and accurate, as it can better fit real-world data where the influence of one variable might change based on the value of another. For instance, the relationship between education level and salary might vary depending on years of experience, which can be modeled through an interaction term. This helps improve model accuracy and provides a deeper understanding of how different predictors jointly affect the outcome. However, while interaction terms enhance the model‚Äôs ability to capture complex relationships, they should be used carefully to avoid overfitting, especially when dealing with a large number of predictors.\n"
      ],
      "metadata": {
        "id": "w-fmtiAc9RBf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15.\n",
        "\n",
        "# In Simple Linear Regression, the intercept represents the expected value of the dependent variable when the independent variable is equal to zero. It provides a baseline prediction for the dependent variable when the predictor has no effect. For example, if you're modeling the relationship between hours studied and test score, the intercept would represent the expected test score when hours studied is zero.\n",
        "\n",
        "# In Multiple Linear Regression, the intercept still represents the expected value of the dependent variable when all independent variables are zero. However, the interpretation becomes more nuanced, especially when multiple predictors are involved. The intercept in Multiple Linear Regression is the baseline value of the dependent variable when all the predictors are at their reference level (e.g., zero for continuous variables or the baseline category for categorical variables). Since the model includes multiple variables, the intercept represents a \"starting point\" that combines the effects of all predictors being zero."
      ],
      "metadata": {
        "id": "i3e7_ilV91U9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16.\n",
        "\n",
        "# The slope in regression analysis represents the rate of change in the dependent variable for each unit change in the corresponding independent variable. It quantifies the strength and direction of the relationship between a predictor and the outcome. A positive slope indicates that as the independent variable increases, the dependent variable also increases, while a negative slope suggests that as the independent variable increases, the dependent variable decreases. The slope significantly affects predictions, as it directly influences how the dependent variable is estimated based on the independent variable. In simple linear regression, the predicted value of the dependent variable is calculated by multiplying the slope by the independent variable and adding the intercept. In multiple linear regression, the slope for each independent variable shows the expected change in the dependent variable when that specific predictor changes, while holding all other variables constant. The slope's interpretation can vary depending on the context, providing insights into how different factors influence the outcome. Overall, the slope is crucial for understanding the relationship between variables and making accurate predictions based on the model.\n"
      ],
      "metadata": {
        "id": "bFqPx5HS-FXh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17.\n",
        "\n",
        "# The intercept in a regression model represents the expected value of the dependent variable when all independent variables are zero. It provides a baseline or starting point for the relationship between the variables. In simple linear regression, it shows the value of the dependent variable when the independent variable is zero, while in multiple linear regression, it represents the dependent variable‚Äôs value when all predictors are zero. Although its interpretation can be less meaningful if some predictors cannot realistically be zero, the intercept still offers important context for understanding how the independent variables influence the dependent variable.\n"
      ],
      "metadata": {
        "id": "Dz_q3XdY-izi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18.\n",
        "\n",
        "# While R¬≤ provides useful information about the proportion of variance explained by the model, it should not be used in isolation. Complementing R¬≤ with other metrics like Adjusted R¬≤, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and cross-validation techniques can provide a more comprehensive evaluation of model performance.\n"
      ],
      "metadata": {
        "id": "BahSavVt-zh8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19.\n",
        "\n",
        "# A large standard error for a regression coefficient indicates significant uncertainty about the estimate of that coefficient, meaning the model has less confidence in the relationship between the predictor and the dependent variable. This suggests that the coefficient is not reliably estimated and could vary significantly with different samples of data. A large standard error also reduces the precision of the estimate, making it harder to draw firm conclusions about the predictor's effect. As a result, the t-statistic, which is used to assess statistical significance, becomes smaller, making it more difficult to reject the null hypothesis that the coefficient is zero. This implies that the predictor may not be statistically significant. A large standard error can stem from factors like multicollinearity (high correlation between predictors), a small sample size, or high variance in the data. Overall, it suggests that the model may not be well-specified, and the estimates may be unreliable, requiring further investigation or refinement to improve the model's accuracy.\n"
      ],
      "metadata": {
        "id": "ZLYsiJL-_FcR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20.\n",
        "\n",
        "# Heteroscedasticity can be identified in residual plots by observing patterns in the spread of residuals. In a residuals vs. fitted values plot, heteroscedasticity is often indicated by a funnel-shaped or fan-shaped pattern, where the spread of residuals increases or decreases as the fitted values change. Similarly, the scale-location plot, which shows the square root of the absolute residuals against the fitted values, can reveal such patterns. Detecting heteroscedasticity is important because it affects statistical inference. When heteroscedasticity is present, standard errors of the regression coefficients can become unreliable, leading to incorrect conclusions in hypothesis testing. Although the regression coefficients remain unbiased, they become less precise, making the model‚Äôs predictions less trustworthy. To address heteroscedasticity, transformations of the dependent or independent variables, such as logarithmic transformations, or using techniques like weighted least squares or robust standard errors, can help stabilize the variance of residuals and improve model accuracy. Addressing this issue ensures more reliable estimates and enhances the validity of the regression analysis."
      ],
      "metadata": {
        "id": "GzT3x2MT_2PZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21.\n",
        "\n",
        "# If a Multiple Linear Regression model has a high R¬≤ but a low adjusted R¬≤, it often indicates overfitting. While a high R¬≤ suggests that the model explains a large portion of the variance in the dependent variable, it can be misleading because R¬≤ always increases with the addition of more predictors, even if those predictors don‚Äôt meaningfully contribute to the model. On the other hand, the adjusted R¬≤ accounts for the number of predictors and penalizes the inclusion of irrelevant variables. A large gap between R¬≤ and adjusted R¬≤ suggests that the additional predictors may be unnecessary, capturing noise rather than useful patterns, which leads to overfitting. This combination implies that the model may fit the training data well but struggle to generalize to new, unseen data. To improve the model, it‚Äôs important to reconsider the choice of predictors, remove irrelevant variables, or use techniques like regularization or cross-validation to ensure the model generalizes better to new data.\n"
      ],
      "metadata": {
        "id": "xtch_WuwAYmw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22.\n",
        "\n",
        "# Scaling variables in Multiple Linear Regression is crucial because it ensures that the model treats all predictors equally, especially when they are on different scales. Variables with different units and magnitudes can bias the model, as it may give disproportionate weight to variables with larger scales. Scaling ensures that each predictor contributes equally to the model, allowing for a more balanced and accurate analysis. Additionally, scaling improves the interpretability of the regression coefficients, making it easier to compare their effects on the dependent variable. It also aids in faster and more efficient optimization, as gradient-based algorithms perform better when variables are on similar scales. Furthermore, scaling can help mitigate issues like multicollinearity, where highly correlated predictors can distort the model, and is essential for regularization techniques such as Ridge and Lasso, which rely on uniform penalties for the coefficients. Overall, scaling ensures that the model is stable, efficient, and interpretable, leading to more reliable results.\n"
      ],
      "metadata": {
        "id": "hAVGbgUzA-kO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23.\n",
        "\n",
        "# Polynomial regression is an extension of linear regression that allows you to model more complex, non-linear relationships by including polynomial terms in the regression equation. While it increases the model's flexibility, careful consideration of the degree of the polynomial is important to avoid overfitting and ensure that the model generalizes well to new data.\n"
      ],
      "metadata": {
        "id": "J3rTfODSBPmQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24.\n",
        "\n",
        "# Polynomial regression and linear regression are both methods for modeling relationships between variables, but they differ in how they handle the nature of these relationships. Linear regression assumes a straight-line relationship between the dependent variable and the independent variable(s), represented by a simple linear equation. It suggests that for each unit increase in the independent variable, the dependent variable changes by a constant amount. On the other hand, polynomial regression extends linear regression by adding higher-order terms (like X^2,X^3,‚Ä¶) to the equation, allowing the model to capture non-linear relationships. This makes polynomial regression more flexible and able to model curved or more complex relationships between variables. However, this flexibility comes with a risk of overfitting, especially when the polynomial degree is too high, as the model may fit the noise in the data rather than the underlying pattern. Additionally, while linear regression provides a straightforward interpretation of the coefficients, polynomial regression can make interpretation more complex, as each term reflects different aspects of the relationship. Overall, polynomial regression is better suited for modeling non-linear relationships, whereas linear regression is simpler and effective for linear trends."
      ],
      "metadata": {
        "id": "6NL3_nrTBWtZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25.\n",
        "\n",
        "# Polynomial regression is used when the relationship between the independent and dependent variables is non-linear, and a simple linear model cannot adequately capture the patterns in the data. It is particularly useful when the data shows curved or parabolic trends, such as when the effect of the independent variable on the dependent variable accelerates or decelerates at different levels. For instance, polynomial regression can model U-shaped or inverted U-shaped relationships, commonly seen in fields like economics or biology, where variables such as age and productivity might follow such patterns. It is also beneficial when you want to avoid overfitting with more complex models like decision trees or neural networks. Additionally, polynomial regression is often used in experimental data where physical laws or trends follow polynomial relationships, such as in physics or chemistry. By allowing for greater flexibility than linear regression, polynomial regression can better fit non-linear data, though care must be taken to avoid overfitting, especially with higher-degree polynomials.\n"
      ],
      "metadata": {
        "id": "CqgaZV3UB_P_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26.\n",
        "\n",
        "# The general equation for polynomial regression is:\n",
        "# Y=Œ≤0 + Œ≤1 X + Œ≤2 X^2 + Œ≤3 X^3 +‚ãØ+ Œ≤n X^n + œµ\n",
        "\n",
        "# Where:\n",
        "# - Y is the dependent variable,\n",
        "# - X is the independent variable,\n",
        "# - Œ≤0,Œ≤1,‚Ä¶,Œ≤n are the coefficients,\n",
        "# - X^2,X^3,‚Ä¶,X^n are the polynomial terms,œµ is the error term.\n",
        "\n",
        "# This equation models a non-linear relationship by adding higher-degree terms of X to the regression."
      ],
      "metadata": {
        "id": "xphs-6xSChlE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q27.\n",
        "\n",
        "# Yes, polynomial regression can be applied to multiple variables. When used with multiple independent variables, the equation extends to include polynomial terms for each variable and their interactions. For example, with two variables X1 and X2, the polynomial regression equation would look like:\n",
        "\n",
        "# Y=Œ≤0 +Œ≤1 X1+Œ≤2 X2+Œ≤3 X1^2+Œ≤4 X2^2+Œ≤5 X1 X2+....+œµ\n",
        "\n",
        "\n",
        "# This allows the model to capture non-linear relationships not just for individual variables but also their interactions."
      ],
      "metadata": {
        "id": "6_qSK-5kDmYY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28.\n",
        "\n",
        "# The limitations of polynomial regression include:\n",
        "\n",
        "# Overfitting: Higher-degree polynomials can fit the training data too closely, capturing noise and leading to poor generalization to new data.\n",
        "\n",
        "# Complexity: As the degree of the polynomial increases, the model becomes more complex and harder to interpret.\n",
        "\n",
        "# Extrapolation Issues: Polynomial models can behave unpredictably outside the range of the training data, leading to unreliable predictions.\n",
        "\n",
        "# Multicollinearity: Higher-degree terms (e.g.,X^2,X^3) can be highly correlated with the original variable, causing instability in the model's coefficients.\n",
        "\n",
        "# Sensitive to Outliers: Polynomial regression is sensitive to outliers, which can distort the model‚Äôs fit."
      ],
      "metadata": {
        "id": "wvdW1P8DEXq4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29.\n",
        "\n",
        "# To evaluate model fit when selecting the degree of a polynomial, you can use:\n",
        "\n",
        "# Cross-validation: Helps assess model generalization by testing on different data subsets.\n",
        "\n",
        "# R-squared (R¬≤): Measures variance explained by the model, but may increase with higher-degree polynomials.\n",
        "\n",
        "# Adjusted R-squared: Accounts for model complexity and helps identify the best degree.\n",
        "\n",
        "# AIC/BIC: Penalize for too many parameters, helping balance fit and complexity.\n",
        "\n",
        "# Residual analysis: Examines prediction errors to detect underfitting or overfitting.\n",
        "\n",
        "# MSE/RMSE: Measures prediction accuracy, with lower values indicating better fit.\n",
        "\n",
        "# Visual inspection: Plotting predictions versus actual data to check for overfitting or underfitting."
      ],
      "metadata": {
        "id": "stB5qCsxEnvG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30.\n",
        "\n",
        "# Visualization is crucial in polynomial regression because it allows you to assess how well the polynomial model fits the data. By plotting the polynomial curve alongside the actual data points, you can visually inspect whether the model captures the underlying relationship. This helps in identifying issues like overfitting, where the model becomes overly complex and fits noise in the data, or underfitting, where the model fails to capture important trends. Additionally, visualization of the residuals (the differences between observed and predicted values) can reveal patterns or biases in the model, guiding adjustments. For higher-degree polynomials, it also aids in understanding the complexity of the relationships between variables. Furthermore, visualizing the polynomial curve can highlight how the model behaves outside the range of the data, which is important for evaluating the reliability of predictions in those regions. In short, visualization enhances the understanding of model performance and helps in making more informed decisions during model selection and interpretation.\n"
      ],
      "metadata": {
        "id": "ISa3wK2mEz7Q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q31.\n",
        "\n",
        "# To implement polynomial regression in Python, follow these steps:\n",
        "\n",
        "# Import Libraries: Use numpy, matplotlib, and scikit-learn.\n",
        "\n",
        "# Prepare Data: Define your independent variable X and dependent variable Y.\n",
        "\n",
        "# Create Polynomial Features: Use PolynomialFeatures to transform X into polynomial features.\n",
        "\n",
        "# Train Model: Fit the polynomial features to a linear regression model.\n",
        "\n",
        "# Visualize: Plot the original data and the polynomial regression curve.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([1, 4, 9, 16, 25])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, Y, color='blue')\n",
        "plt.plot(X, Y_pred, color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "e--xVvCmE_vu",
        "outputId": "100d3030-5eb3-41e2-e049-241e7681d58d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}